{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3c2403-3e5b-4260-b126-e4d16d65b123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (0.21.0)\n",
      "Requirement already satisfied: scikit-image in c:\\programdata\\anaconda3\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (1.13.1)\n",
      "Requirement already satisfied: imageio>=2.33 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=21 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python numpy torch torchvision scikit-image pillow scikit-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1affe1d-d627-4eff-8cf4-f5f57fc13b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa34c8dc-ff92-4bbb-8103-6b9024bcb37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 4289\n",
      "Processing batch 1/215\n",
      "Processing batch 2/215\n",
      "Processing batch 3/215\n",
      "Processing batch 4/215\n",
      "Processing batch 5/215\n",
      "Processing batch 6/215\n",
      "Processing batch 7/215\n",
      "Processing batch 8/215\n",
      "Processing batch 9/215\n",
      "Processing batch 10/215\n",
      "Processing batch 11/215\n",
      "Processing batch 12/215\n",
      "Processing batch 13/215\n",
      "Processing batch 14/215\n",
      "Processing batch 15/215\n",
      "Processing batch 16/215\n",
      "Processing batch 17/215\n",
      "Processing batch 18/215\n",
      "Processing batch 19/215\n",
      "Processing batch 20/215\n",
      "Processing batch 21/215\n",
      "Processing batch 22/215\n",
      "Processing batch 23/215\n",
      "Processing batch 24/215\n",
      "Processing batch 25/215\n",
      "Processing batch 26/215\n",
      "Processing batch 27/215\n",
      "Processing batch 28/215\n",
      "Processing batch 29/215\n",
      "Processing batch 30/215\n",
      "Processing batch 31/215\n",
      "Processing batch 32/215\n",
      "Processing batch 33/215\n",
      "Processing batch 34/215\n",
      "Processing batch 35/215\n",
      "Processing batch 36/215\n",
      "Processing batch 37/215\n",
      "Processing batch 38/215\n",
      "Processing batch 39/215\n",
      "Processing batch 40/215\n",
      "Processing batch 41/215\n",
      "Processing batch 42/215\n",
      "Processing batch 43/215\n",
      "Processing batch 44/215\n",
      "Processing batch 45/215\n",
      "Processing batch 46/215\n",
      "Processing batch 47/215\n",
      "Processing batch 48/215\n",
      "Processing batch 49/215\n",
      "Processing batch 50/215\n",
      "Processing batch 51/215\n",
      "Processing batch 52/215\n",
      "Processing batch 53/215\n",
      "Processing batch 54/215\n",
      "Processing batch 55/215\n",
      "Processing batch 56/215\n",
      "Processing batch 57/215\n",
      "Processing batch 58/215\n",
      "Processing batch 59/215\n",
      "Processing batch 60/215\n",
      "Processing batch 61/215\n",
      "Processing batch 62/215\n",
      "Processing batch 63/215\n",
      "Processing batch 64/215\n",
      "Processing batch 65/215\n",
      "Processing batch 66/215\n",
      "Processing batch 67/215\n",
      "Processing batch 68/215\n",
      "Processing batch 69/215\n",
      "Processing batch 70/215\n",
      "Processing batch 71/215\n",
      "Processing batch 72/215\n",
      "Processing batch 73/215\n",
      "Processing batch 74/215\n",
      "Processing batch 75/215\n",
      "Processing batch 76/215\n",
      "Processing batch 77/215\n",
      "Processing batch 78/215\n",
      "Processing batch 79/215\n",
      "Processing batch 80/215\n",
      "Processing batch 81/215\n",
      "Processing batch 82/215\n",
      "Processing batch 83/215\n",
      "Processing batch 84/215\n",
      "Processing batch 85/215\n",
      "Processing batch 86/215\n",
      "Processing batch 87/215\n",
      "Processing batch 88/215\n",
      "Processing batch 89/215\n",
      "Processing batch 90/215\n",
      "Processing batch 91/215\n",
      "Processing batch 92/215\n",
      "Processing batch 93/215\n",
      "Processing batch 94/215\n",
      "Processing batch 95/215\n",
      "Processing batch 96/215\n",
      "Processing batch 97/215\n",
      "Processing batch 98/215\n",
      "Processing batch 99/215\n",
      "Processing batch 100/215\n",
      "Processing batch 101/215\n",
      "Processing batch 102/215\n",
      "Processing batch 103/215\n",
      "Processing batch 104/215\n",
      "Processing batch 105/215\n",
      "Processing batch 106/215\n",
      "Processing batch 107/215\n",
      "Processing batch 108/215\n",
      "Processing batch 109/215\n",
      "Processing batch 110/215\n",
      "Processing batch 111/215\n",
      "Processing batch 112/215\n",
      "Processing batch 113/215\n",
      "Processing batch 114/215\n",
      "Processing batch 115/215\n",
      "Processing batch 116/215\n",
      "Processing batch 117/215\n",
      "Processing batch 118/215\n",
      "Processing batch 119/215\n",
      "Processing batch 120/215\n",
      "Processing batch 121/215\n",
      "Processing batch 122/215\n",
      "Processing batch 123/215\n",
      "Processing batch 124/215\n",
      "Processing batch 125/215\n",
      "Processing batch 126/215\n",
      "Processing batch 127/215\n",
      "Processing batch 128/215\n",
      "Processing batch 129/215\n",
      "Processing batch 130/215\n",
      "Processing batch 131/215\n",
      "Processing batch 132/215\n",
      "Processing batch 133/215\n",
      "Processing batch 134/215\n",
      "Processing batch 135/215\n",
      "Processing batch 136/215\n",
      "Processing batch 137/215\n",
      "Processing batch 138/215\n",
      "Processing batch 139/215\n",
      "Processing batch 140/215\n",
      "Processing batch 141/215\n",
      "Processing batch 142/215\n",
      "Processing batch 143/215\n",
      "Processing batch 144/215\n",
      "Processing batch 145/215\n",
      "Processing batch 146/215\n",
      "Processing batch 147/215\n",
      "Processing batch 148/215\n",
      "Processing batch 149/215\n",
      "Processing batch 150/215\n",
      "Processing batch 151/215\n",
      "Processing batch 152/215\n",
      "Processing batch 153/215\n",
      "Processing batch 154/215\n",
      "Processing batch 155/215\n",
      "Processing batch 156/215\n",
      "Processing batch 157/215\n",
      "Processing batch 158/215\n",
      "Processing batch 159/215\n",
      "Processing batch 160/215\n",
      "Processing batch 161/215\n",
      "Processing batch 162/215\n",
      "Processing batch 163/215\n",
      "Processing batch 164/215\n",
      "Processing batch 165/215\n",
      "Processing batch 166/215\n",
      "Processing batch 167/215\n",
      "Processing batch 168/215\n",
      "Processing batch 169/215\n",
      "Processing batch 170/215\n",
      "Processing batch 171/215\n",
      "Processing batch 172/215\n",
      "Processing batch 173/215\n",
      "Processing batch 174/215\n",
      "Processing batch 175/215\n",
      "Processing batch 176/215\n",
      "Processing batch 177/215\n",
      "Processing batch 178/215\n",
      "Processing batch 179/215\n",
      "Processing batch 180/215\n",
      "Processing batch 181/215\n",
      "Processing batch 182/215\n",
      "Processing batch 183/215\n",
      "Processing batch 184/215\n",
      "Processing batch 185/215\n",
      "Processing batch 186/215\n",
      "Processing batch 187/215\n",
      "Processing batch 188/215\n",
      "Processing batch 189/215\n",
      "Processing batch 190/215\n",
      "Processing batch 191/215\n",
      "Processing batch 192/215\n",
      "Processing batch 193/215\n",
      "Processing batch 194/215\n",
      "Processing batch 195/215\n",
      "Processing batch 196/215\n",
      "Processing batch 197/215\n",
      "Processing batch 198/215\n",
      "Processing batch 199/215\n",
      "Processing batch 200/215\n",
      "Processing batch 201/215\n",
      "Processing batch 202/215\n",
      "Processing batch 203/215\n",
      "Processing batch 204/215\n",
      "Processing batch 205/215\n",
      "Processing batch 206/215\n",
      "Processing batch 207/215\n",
      "Processing batch 208/215\n",
      "Processing batch 209/215\n",
      "Processing batch 210/215\n",
      "Processing batch 211/215\n",
      "Processing batch 212/215\n",
      "Processing batch 213/215\n",
      "Processing batch 214/215\n",
      "Processing batch 215/215\n",
      "Extracted 4289 HOG and 4289 VGG16 features\n",
      "Feature extraction complete! Saved as 'hog_vgg16_features_reduced.npy'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.feature import hog\n",
    "from PIL import Image\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib  # For saving intermediate files\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset Path\n",
    "root_dir = r\"D:\\Signature verification\\Signature verification\\full\"\n",
    "\n",
    "file_paths = []\n",
    "labels = []\n",
    "\n",
    "# Read images and labels\n",
    "for class_name in os.listdir(root_dir):\n",
    "    class_path = os.path.join(root_dir, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        for file in os.listdir(class_path):\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                file_paths.append(os.path.join(class_path, file))\n",
    "                labels.append(class_name)\n",
    "\n",
    "print(f\"Total images: {len(file_paths)}\")\n",
    "\n",
    "# Load VGG16 for feature extraction\n",
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).to(device)\n",
    "vgg16.eval()\n",
    "\n",
    "# Remove classifier layers\n",
    "vgg16_extractor = nn.Sequential(*list(vgg16.children())[:-1]).to(device)\n",
    "\n",
    "# Image transformation (Resized to 224x224 for VGG16)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Process images in batches (Reduce memory usage)\n",
    "batch_size = 20  \n",
    "\n",
    "hog_features_list = []\n",
    "vgg16_features_list = []\n",
    "\n",
    "for i in range(0, len(file_paths), batch_size):\n",
    "    print(f\"Processing batch {i//batch_size + 1}/{(len(file_paths)//batch_size) + 1}\")\n",
    "\n",
    "    batch_hog = []\n",
    "    batch_vgg16 = []\n",
    "\n",
    "    for image_path in file_paths[i:i + batch_size]:\n",
    "        # Load Image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Warning: Could not load {image_path}\")\n",
    "            continue\n",
    "\n",
    "        # Convert to grayscale for HOG\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Extract HOG Features (Reduce size for efficiency)\n",
    "        hog_features = hog(gray, pixels_per_cell=(8, 8), cells_per_block=(1, 1), feature_vector=True)\n",
    "        batch_hog.append(hog_features.astype(np.float16))  # Use float16 to save memory\n",
    "\n",
    "        # Convert image for VGG16\n",
    "        image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        image_pil = transform(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "        # Extract VGG16 features\n",
    "        with torch.no_grad():\n",
    "            vgg16_feature = vgg16_extractor(image_pil).cpu().numpy().flatten()\n",
    "\n",
    "        batch_vgg16.append(vgg16_feature.astype(np.float16))  # Use float16\n",
    "\n",
    "    hog_features_list.extend(batch_hog)\n",
    "    vgg16_features_list.extend(batch_vgg16)\n",
    "\n",
    "    # Save intermediate features to disk (Reduce RAM usage)\n",
    "    joblib.dump(batch_hog, f\"hog_batch_{i//batch_size}.pkl\")\n",
    "    joblib.dump(batch_vgg16, f\"vgg16_batch_{i//batch_size}.pkl\")\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "print(f\"Extracted {len(hog_features_list)} HOG and {len(vgg16_features_list)} VGG16 features\")\n",
    "\n",
    "# Find max HOG feature length\n",
    "max_hog_length = max(len(f) for f in hog_features_list)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "hog_features_padded = np.zeros((len(hog_features_list), max_hog_length), dtype=np.float16)\n",
    "for i in range(len(hog_features_list)):\n",
    "    hog_features_padded[i, :len(hog_features_list[i])] = hog_features_list[i]\n",
    "\n",
    "vgg16_features_array = np.array(vgg16_features_list, dtype=np.float16)\n",
    "\n",
    "# Concatenate HOG + VGG16 features\n",
    "features = np.hstack((hog_features_padded, vgg16_features_array))\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=500)  # Reduce to 500 components for efficiency\n",
    "features = pca.fit_transform(features)\n",
    "\n",
    "# Save reduced features to disk\n",
    "np.save(\"hog_vgg16_features_reduced.npy\", features)\n",
    "\n",
    "print(\"Feature extraction complete! Saved as 'hog_vgg16_features_reduced.npy'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aead85a0-7815-48c1-b819-b7afaf864e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9161\n"
     ]
    }
   ],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels[:len(features)])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate Model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc637ef6-a239-4a08-ab1c-a041ac05b9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 feature vector size: 25088\n"
     ]
    }
   ],
   "source": [
    "print(f\"VGG16 feature vector size: {vgg16_features_array.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad2bcd44-e1fc-451b-b03f-15b867532b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max HOG feature length: 149283\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max HOG feature length: {max_hog_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9567269c-7f91-4828-ab2f-74367d06b694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA model saved as 'pca_model.pkl'\n",
      "Random Forest model saved as 'random_forest_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save PCA model\n",
    "joblib.dump(pca, \"pca_model.pkl\")\n",
    "print(\"PCA model saved as 'pca_model.pkl'\")\n",
    "\n",
    "# Save Random Forest model\n",
    "joblib.dump(rf_classifier, \"random_forest_model.pkl\")\n",
    "print(\"Random Forest model saved as 'random_forest_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cea7624-8442-4951-8f45-a648bcd48b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91         5\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       1.00      1.00      1.00         7\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       0.83      1.00      0.91         5\n",
      "           5       1.00      0.50      0.67         4\n",
      "           6       0.80      1.00      0.89         4\n",
      "           7       1.00      0.50      0.67         2\n",
      "           8       0.73      1.00      0.84         8\n",
      "           9       1.00      1.00      1.00         3\n",
      "          10       0.88      1.00      0.93         7\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       1.00      1.00      1.00         6\n",
      "          13       1.00      0.67      0.80         3\n",
      "          14       0.75      1.00      0.86         3\n",
      "          15       1.00      0.67      0.80         3\n",
      "          16       0.45      1.00      0.62         5\n",
      "          17       1.00      0.29      0.44         7\n",
      "          18       1.00      1.00      1.00         8\n",
      "          20       0.86      1.00      0.92         6\n",
      "          21       0.14      1.00      0.25         1\n",
      "          22       0.33      1.00      0.50         1\n",
      "          23       0.00      0.00      0.00         2\n",
      "          24       1.00      0.75      0.86         4\n",
      "          25       0.50      1.00      0.67         1\n",
      "          26       0.67      1.00      0.80         2\n",
      "          27       0.00      0.00      0.00         1\n",
      "          28       1.00      1.00      1.00         4\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       1.00      1.00      1.00         4\n",
      "          31       1.00      1.00      1.00         4\n",
      "          32       0.00      0.00      0.00         0\n",
      "          33       1.00      0.75      0.86         4\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         1\n",
      "          36       0.60      1.00      0.75         3\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      1.00      1.00         3\n",
      "          39       1.00      0.33      0.50         3\n",
      "          40       1.00      1.00      1.00         1\n",
      "          41       1.00      1.00      1.00         3\n",
      "          43       1.00      0.33      0.50         3\n",
      "          44       1.00      1.00      1.00         2\n",
      "          45       1.00      1.00      1.00         1\n",
      "          46       1.00      1.00      1.00         4\n",
      "          47       1.00      1.00      1.00         3\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       1.00      1.00      1.00         2\n",
      "          50       0.67      1.00      0.80         2\n",
      "          51       1.00      0.50      0.67         2\n",
      "          52       1.00      1.00      1.00         3\n",
      "          53       1.00      0.75      0.86         4\n",
      "          54       1.00      1.00      1.00         1\n",
      "          55       1.00      1.00      1.00         3\n",
      "          56       1.00      0.80      0.89         5\n",
      "          57       1.00      1.00      1.00         4\n",
      "          59       1.00      1.00      1.00         3\n",
      "          60       0.50      1.00      0.67         1\n",
      "          61       1.00      0.33      0.50         3\n",
      "          62       0.67      1.00      0.80         2\n",
      "          63       1.00      0.75      0.86         4\n",
      "          64       0.50      1.00      0.67         3\n",
      "          65       1.00      0.20      0.33         5\n",
      "          66       0.00      0.00      0.00         0\n",
      "          67       0.25      1.00      0.40         1\n",
      "          68       1.00      1.00      1.00         1\n",
      "          69       1.00      1.00      1.00         1\n",
      "          70       1.00      1.00      1.00         2\n",
      "          71       1.00      1.00      1.00         1\n",
      "          72       0.33      1.00      0.50         1\n",
      "          73       1.00      0.25      0.40         4\n",
      "          74       0.50      0.50      0.50         2\n",
      "          75       1.00      0.25      0.40         4\n",
      "          76       1.00      0.80      0.89         5\n",
      "          77       0.00      0.00      0.00         4\n",
      "          78       0.60      1.00      0.75         3\n",
      "          79       0.67      0.50      0.57         4\n",
      "          80       1.00      1.00      1.00         1\n",
      "          81       0.50      1.00      0.67         1\n",
      "          82       1.00      1.00      1.00         5\n",
      "          83       1.00      1.00      1.00         1\n",
      "          84       1.00      1.00      1.00         1\n",
      "          86       1.00      1.00      1.00         2\n",
      "          87       1.00      1.00      1.00         4\n",
      "          88       1.00      1.00      1.00         6\n",
      "          89       1.00      1.00      1.00         4\n",
      "          90       1.00      1.00      1.00         3\n",
      "          91       1.00      0.50      0.67         2\n",
      "          92       1.00      1.00      1.00         5\n",
      "          93       1.00      1.00      1.00         2\n",
      "          94       0.33      1.00      0.50         1\n",
      "          95       1.00      0.67      0.80         6\n",
      "          96       0.40      1.00      0.57         2\n",
      "          97       1.00      0.50      0.67         6\n",
      "          98       1.00      1.00      1.00         2\n",
      "          99       1.00      1.00      1.00         1\n",
      "         100       1.00      1.00      1.00         3\n",
      "         101       1.00      1.00      1.00         2\n",
      "         102       1.00      1.00      1.00         2\n",
      "         103       1.00      1.00      1.00         2\n",
      "         104       0.67      1.00      0.80         2\n",
      "         105       1.00      0.75      0.86         4\n",
      "         106       1.00      1.00      1.00         4\n",
      "         107       1.00      0.33      0.50         3\n",
      "         108       1.00      1.00      1.00         2\n",
      "         109       1.00      1.00      1.00         4\n",
      "         110       1.00      1.00      1.00         3\n",
      "         111       1.00      1.00      1.00         2\n",
      "         112       1.00      1.00      1.00         4\n",
      "         113       1.00      0.33      0.50         3\n",
      "         114       0.80      1.00      0.89         4\n",
      "         115       1.00      0.50      0.67         6\n",
      "         116       0.50      1.00      0.67         1\n",
      "         117       1.00      1.00      1.00         1\n",
      "         118       1.00      1.00      1.00         1\n",
      "         119       1.00      1.00      1.00         1\n",
      "         120       1.00      0.60      0.75         5\n",
      "         121       0.67      1.00      0.80         2\n",
      "         122       1.00      1.00      1.00         3\n",
      "         123       1.00      1.00      1.00         1\n",
      "         124       1.00      1.00      1.00         6\n",
      "         125       1.00      1.00      1.00         2\n",
      "         126       1.00      1.00      1.00         3\n",
      "         127       1.00      1.00      1.00         2\n",
      "         128       0.88      1.00      0.93         7\n",
      "         129       1.00      1.00      1.00         2\n",
      "         130       1.00      1.00      1.00         2\n",
      "         131       1.00      1.00      1.00         6\n",
      "         132       1.00      0.80      0.89         5\n",
      "         133       0.86      1.00      0.92         6\n",
      "         134       1.00      1.00      1.00         2\n",
      "         135       1.00      0.75      0.86         4\n",
      "         136       1.00      0.71      0.83         7\n",
      "         137       0.80      1.00      0.89         4\n",
      "         138       1.00      0.80      0.89         5\n",
      "         139       1.00      1.00      1.00         2\n",
      "         140       1.00      1.00      1.00         6\n",
      "         141       1.00      1.00      1.00         6\n",
      "         142       0.75      1.00      0.86         3\n",
      "         143       1.00      0.75      0.86         4\n",
      "         144       1.00      1.00      1.00         1\n",
      "         145       1.00      1.00      1.00         3\n",
      "         146       1.00      1.00      1.00         4\n",
      "         147       1.00      1.00      1.00         7\n",
      "         148       1.00      1.00      1.00         2\n",
      "         149       1.00      1.00      1.00         3\n",
      "         150       1.00      1.00      1.00         4\n",
      "         151       1.00      1.00      1.00         3\n",
      "         152       1.00      1.00      1.00         5\n",
      "         153       1.00      1.00      1.00         5\n",
      "         154       1.00      1.00      1.00         5\n",
      "         155       0.89      1.00      0.94         8\n",
      "         156       1.00      1.00      1.00         4\n",
      "         157       1.00      1.00      1.00         7\n",
      "         158       1.00      1.00      1.00         3\n",
      "         159       1.00      1.00      1.00         7\n",
      "         160       0.88      1.00      0.93         7\n",
      "         161       1.00      1.00      1.00         6\n",
      "         162       1.00      1.00      1.00         6\n",
      "         163       0.67      1.00      0.80         2\n",
      "         164       1.00      1.00      1.00         4\n",
      "         165       0.67      1.00      0.80         4\n",
      "         166       1.00      1.00      1.00         6\n",
      "         167       1.00      1.00      1.00         4\n",
      "         168       1.00      1.00      1.00         5\n",
      "         169       1.00      1.00      1.00         4\n",
      "         170       1.00      1.00      1.00         4\n",
      "         171       1.00      1.00      1.00         4\n",
      "         172       1.00      1.00      1.00         3\n",
      "         173       1.00      1.00      1.00         1\n",
      "         174       1.00      1.00      1.00         2\n",
      "         175       1.00      1.00      1.00         3\n",
      "         177       1.00      1.00      1.00         3\n",
      "         178       1.00      1.00      1.00         3\n",
      "         179       1.00      1.00      1.00         7\n",
      "         180       1.00      1.00      1.00         6\n",
      "         181       1.00      1.00      1.00         3\n",
      "         182       0.67      1.00      0.80         2\n",
      "         183       1.00      1.00      1.00         4\n",
      "         184       1.00      1.00      1.00         5\n",
      "         185       1.00      1.00      1.00         1\n",
      "         186       1.00      1.00      1.00         3\n",
      "         187       1.00      1.00      1.00         4\n",
      "         188       1.00      1.00      1.00         1\n",
      "         189       1.00      1.00      1.00         7\n",
      "         190       1.00      1.00      1.00         3\n",
      "         191       1.00      1.00      1.00         6\n",
      "         192       1.00      1.00      1.00         9\n",
      "         193       1.00      1.00      1.00         6\n",
      "         194       1.00      1.00      1.00         3\n",
      "         195       1.00      1.00      1.00         5\n",
      "         196       1.00      1.00      1.00         6\n",
      "         197       1.00      1.00      1.00         5\n",
      "         198       1.00      1.00      1.00         5\n",
      "         199       1.00      1.00      1.00         2\n",
      "         200       1.00      1.00      1.00         2\n",
      "         201       1.00      1.00      1.00         6\n",
      "         202       1.00      1.00      1.00         3\n",
      "         203       1.00      1.00      1.00         4\n",
      "         204       1.00      1.00      1.00         5\n",
      "         205       1.00      1.00      1.00         6\n",
      "         206       1.00      1.00      1.00         7\n",
      "         207       0.86      1.00      0.92         6\n",
      "         208       1.00      1.00      1.00         7\n",
      "         209       1.00      1.00      1.00         5\n",
      "         210       0.60      1.00      0.75         3\n",
      "         211       1.00      0.67      0.80         3\n",
      "         212       1.00      1.00      1.00         3\n",
      "         213       1.00      1.00      1.00         5\n",
      "         214       1.00      1.00      1.00         3\n",
      "         215       1.00      1.00      1.00         5\n",
      "         216       1.00      1.00      1.00         4\n",
      "         217       1.00      1.00      1.00         4\n",
      "         218       1.00      1.00      1.00         3\n",
      "         219       1.00      1.00      1.00         5\n",
      "         220       1.00      1.00      1.00         8\n",
      "         221       1.00      1.00      1.00         6\n",
      "         222       1.00      1.00      1.00         3\n",
      "         223       1.00      1.00      1.00        11\n",
      "         224       1.00      1.00      1.00         6\n",
      "         225       0.50      1.00      0.67         1\n",
      "         226       1.00      1.00      1.00        11\n",
      "         227       0.83      1.00      0.91         5\n",
      "         228       1.00      1.00      1.00         5\n",
      "         229       1.00      1.00      1.00         4\n",
      "         230       1.00      1.00      1.00         3\n",
      "         231       1.00      1.00      1.00         4\n",
      "         232       1.00      1.00      1.00         2\n",
      "         233       1.00      1.00      1.00         6\n",
      "         234       0.86      1.00      0.92         6\n",
      "         235       0.75      1.00      0.86         3\n",
      "         236       1.00      1.00      1.00         4\n",
      "         237       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.92       858\n",
      "   macro avg       0.89      0.90      0.87       858\n",
      "weighted avg       0.94      0.92      0.91       858\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict labels using the trained Random Forest model\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Generate and print the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae16f5-7a87-4b63-bba9-0f67dabf343c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b20740d-6288-45d2-bef6-9cf8e467b913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e98e3a-1937-4fbf-9473-a7f4489e55ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342cebaf-02fb-420d-9da0-e7fe100950ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
